\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Image Recognition for Enhancing Fake News Detection in the Grover Natural Language Processing Network\\
}

\author{\IEEEauthorblockN{Abdullah Kamal}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Texas State University}\\
San Marcos, TX \\
aai27@txstate.edu}
\and
\IEEEauthorblockN{Zaid Jamal}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Texas State University}\\
San Marcos, TX \\
z\_j33@txstate.edu}
\and
\IEEEauthorblockN{Gabriel Rosales}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Texas State University}\\
San Marcos, TX \\
gmr94@txstate.edu}
\and
\IEEEauthorblockN{\hspace*{3.7cm}Brian Robinson}
\IEEEauthorblockA{\hspace*{3.7cm}\textit{Department of Computer Science} \\
\hspace*{3.7cm}\textit{Texas State University}\\
\hspace*{3.7cm}San Marcos, TX \\
\hspace*{3.7cm}bwr37@txstate.edu}
\and
\IEEEauthorblockN{\hspace*{-1.5cm}Zachary Sotny}
\IEEEauthorblockA{\hspace*{-1.5cm}\textit{Department of Computer Science} \\
\hspace*{-1.5cm}\textit{Texas State University}\\
\hspace*{-1.5cm}San Marcos, TX \\
\hspace*{-1.5cm}zachsotny@txstate.edu}
}
\maketitle

\begin{abstract}
Fake news is a massive issue interfering with the security of factual information. On social media platforms like Twitter, Instagram, and Reddit, it is common for people to post images of news, which are often falsified, to fire up their audiences or promote certain agendas. This being a major security concern, we decided to analyze a fake news detection algorithm, \emph{Grover}, which is a natural language processing (NLP) \emph{neural network} that analyzes news articles to determine whether they were developed by a human or machine. After testing it on articles generated by some common Artificial Intelligence models like ChatGPT and Ryter, we noticed an inconsistency in its generation of outputs. In many cases, Grover was unable to recognize that a machine had generated the article. Additionally, in the case of social media posts, it was incapable to scan the images for text, making it impractical for usage beyond a very crude technique of manual input of an article. To counteract this flaw, we propose the our very own \emph{Image Recognition for Detecting Misinformation} (IRDM). In essence, IRDM is a program that utilizes the Google Cloud Vision \emph{API} to analyze a data set of images pulled from various popular social media platforms to compare with actual news articles. For testing purposes, we are using the NYTimes Developers API for factual accuracy. The desired output of our project is a ground truth by which Grover can more accurate recognize an article's legitimacy. 
\end{abstract}

\begin{IEEEkeywords}
Grover, Image Recognition for Detecting Misinformation, natural language processing, social media, neural network, API, image-to-text
\end{IEEEkeywords}

\section{Introduction}
Fake news is prevalent on social media, making it a major concern for people looking to find factual evidence. According to Pew Research \cite{pew2021news}, around 50\% of social media users utilize some platform to acquire the latest news. As such, we felt the need to approach this problem utilizing already accessible resources, in this case Grover, along with our own algorithm that would increase the accuracy of recognition. Grover has the ability to recognize a wide variety of fake news articles, but with social media often having information shared via images [Fig.~\ref{fig:fakenews1}], we decided on an approach other than a neural network.   

\begin{figure}
\centering
    \includegraphics[width=0.4\textwidth]{fakenews1.jpg}
    \includegraphics[width=0.4\textwidth]{fakenews2.jpg}
    \caption{Fake News Article Images From Twitter}
\label{fig:fakenews1}
\end{figure}

\section{Solution}

\subsection{Initial Approach}

In the initial stages of development, we proposed the use of a convolutional neural network (CNN) that would be trained on a completely different set of articles. Using the Soft Max activation function, we initially aimed to test the CNN on a collection of articles scraped from popular news outlets on the web and articles auto generated from ChatGPT. However, upon realizing the extent to the size of the CSV's needed to contain the information that would train the CNN, and the time needed to train the model, we decided to revisit our solution. 


\subsection{Image Recognition Extension}

Our finalized approach, IRDM, uses image recognition and \emph{image-to-text} capabilities of the Google Cloud Vision API to analyze content published by users on various social media platforms to recognize it as real and fake news. It will receive an image of a news article and it will parse the text and return the content using a json response. The text is then used in a search in the NYTimes database using their Developers API. If the news article or its content can be found in the database, our program will deem the content of the image as real news; otherwise, it will output an alert that the content may be an instance of fake news. This same technique can be implemented using other news outlets for increased accuracy as one source can never covers all news. However, with the limited time we had to conduct our tests, we developed a proof of concept that worked with New York Times specifically. On top of news validation, our program aims to use our solution as an extension of the preexisting Grover model. In the future, we hope to be able to increase the scope of what Grover can analyze as well as add to the ground truth of the Grover model. By ground truth, we mean coming up with a predetermined answer to each test on Grover so there is a comparative value to work with, thus increasing the accuracy of the already trained NLP network. 

To enable users to easily utilize our news detection program, we created a dynamic HTML multi-page website to complement the image recognition software and increase the user friendliness of our solution. While Grover is not particularly user friendly, we plan on having our open-source solution as an easy way for people to check to see if an uploaded image on social media is factual or not.

Our website is comprised of a page containing the profiles of our team that made this project possible. It also includes a basic overview of what our software does and the main page which collects images as input to validate he content using online news services. 

\section{Results}

Testing IRDM on manually generated New York Times articles with machine and human generated stories, we consistently received an output of fake news when cross-referencing with existing news articles in the NY Times database. To further validate the accuracy of our solution, we needed to acquire a positive for real articles. To do this, we tested our program on images of a variety of news articles pulled from the NY Times database. IRDM was able to validate these articles as factual. We predict that any further tests with both real and fake articles will produce the desired answer so long as the topic of the news was covered by NY Times at some point in time andwe believe that IRDM would best be served as an extension to social media platforms to stem the spread of fake news prior to its posting.

We are confident that our solution is an implementable extension for Grover and will provide a much needed boost to its accuracy, thus dealing with the security concerns related to its inability to consistently recognize fake news. Because our solution focuses more on fact-checking with actual news outlets rather than Grover's approach of determining the likelihood of an article being machine generated, we expand on Grover and its capabilities. Rather than replace Grover or change its source code and retrain its network, the aim of our solution is to allow users to fact check any information they found on various social media platforms, and with the testing we have done, our Image Recognition for Detecting Misinformation is a great way to ensure that that is done.

\section{Limitations}

The biggest limitation with our solution is that it is not directly connected to Grover. Instead, we have a completely separate program that does an extension of what Grover does. With the time we had, we were unable to get to figuring out how to integrate our solution into Grover's source code. We are confident that such a thing can be done, but Grover lacks a friendly user interface that would properly collaborate with ours. Hence, developing a similar interface that would work to implement both components would be best for our intended approach for integration. 

Regarding the actual program itself, IRDM heavily relies on the APIs of both Google Cloud Vision and NY Times Developers and both have their limitations. Google Cloud Vision required a login and a payment plan, which is detrimental to the accessibility of our solution. Additionally, it limits how much can be done and for how long. The NY Times Developers API presents a more in the moment problem with its limitations on API calls, amounting to five every minute and no more than 500 calls in a day. While it is generous, that severely limits the traffic that our program can receive at any given time. Future work would include figuring out an API independent solution.

\section{Acknowledgements}

We would like to thank Dr. Heena Rathore for working with us to develop an implementable solution in the limited time we had prior to submission. We would also like to thank the developers of Grover for providing us with the ideas needed to produce our solution. 

\begin{thebibliography}{00}
\bibitem{pew2021news} Pew Research Center. (2021, September 20). News consumption across social media in 2021. Retrieved April 9, 2023, from \url{https://www.pewresearch.org/journalism/2021/09/20/news-consumption-across-social-media-in-2021/}
\end{thebibliography}
\vspace{12pt}

\end{document}
